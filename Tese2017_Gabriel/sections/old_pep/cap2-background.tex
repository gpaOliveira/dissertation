\chapter{\label{chap:chap4}{Theoretical Background}}

In order to study BDD scenarios quality we first need to understand where requirements quality came from and how to evaluate it. This chapter introduces some concepts around those topics in order to answer some informal questions that comes prior to our research topic: what requirements formats already have solid quality definitions, what are the most common evaluation techniques, what agile requirements representations exists, and how BDD is connected with it all.     

\section{Traditional Requirements}

A requirement is either a condition or capacity necessary to solve a problem or reach a goal for an interested party or some characteristic that a solution or component should possess or acquire in order to fulfill some form of contract \cite{Babok_2009}. Additionally, a requirement is also a written representation of this condition or capacity, or a usable representation of a need focused on understanding what kind of value could be delivered to a client if a requirement is fulfilled \cite{Babok_2015}. 

When classified according to their purpose, they can be called \textbf{business requirements}, \textbf{stakeholder requirements} and \textbf{solution requirements}. The first are statements of goals, objectives, and outcomes that describe why a change has been initiated, while the second are the needs of stakeholders that must be met in order to fulfill business requirements. The later describe the capabilities and qualities of a solution and provide the appropriate level of details to allow the proper implementation of a solution and can be divided into functional requirements (that describes the capabilities a solution must have in terms of the behaviour and information to manage) and non-functional requirements (that describes conditions under which a solution must remain effective). 

\subsection{Use Cases}

Cockburn \cite{Cockburn_2000} says that a use case captures a contract between the stakeholders of a system about its behavior and describes the system’s behavior under various conditions by interacting with one of the stakeholders (the \textit{primary actor}, who want to perform an action and achieve a certain goal). Different sequences of behavior, or scenarios, can unfold, and the use case collects together those different scenarios. They are used to express behavioral requirements for software systems and can be put into service to stimulate discussion within a team about an upcoming system. They might later use that the use case form to document the actual requirements. Besides the primary actor, that interacts with the system, a use case has other parts as well: the \textit{scope} identifies the system that we are discussing, the \textit{preconditions} and \textit{guarantees} say what must be true before and after the use case runs, the \textit{main success scenario} is a case in which nothing goes wrong and the \textit{extensions section} describes what can happen differently during that scenario.

\subsection{Traditional Requirements Quality}

Requirements validation is a phase on traditional requirements engineering process that is known to support the three other activities (requirements elicitation, requirements analysis and requirements specification) by identifying and correcting errors in the requirements \cite{Heikkila_2015}. G{\'e}nova \cite{Genova_2013} says that quality indicators must not provide numerical evaluations only, but first of all they must point out concrete defects and provide suggestions for improvement, just like a spell and grammar checker can help to improve the quality of a text. For \cite{Davis_1993} a perfect software requirements specification is impossible, as some qualities may be achieved only on the expense of others. The author also implies that one must be careful to recognize that although quality is attainable, perfection is not.

The Business Analyst Body of Knowledge (BABOK) \cite{Babok_2015} says that, while quality is ultimately determined by the needs of the stakeholders who will use the requirements or the designs, acceptable quality requirements exhibit many characteristics. The second edition \cite{Babok_2009} describes eight characteristics a requirement must have in order to be a quality one, as follows: cohesion, completeness, consistency, correction, viability, adaptability, unambiguity and testability. The third edition \cite{Babok_2015} bring nine: atomic, complete, consistent, concise, feasible, unambiguous, testable, prioritized and understandable. Both editions \cite{Babok_2009} \cite{Babok_2015} define what each characteristic means, but does not provide any measurement guidance.

G{\'e}nova \cite{Genova_2013} lists other 11 properties along with analytical metrics that would later help the authors to build a quality framework and implement it on the requirements quality analyzer tool. The characteristics are as follows: atomicity, precision, completeness, consistency, understandability, unambiguity, traceability, abstraction, validability, verifiability, modifiability. The measurable indicators are: size, readability, punctuation, acronyms and abbreviations, connective terms, imprecise terms, design terms, imperative verbs, conditional verbs, passive voice, domain terms, versions, nesting, dependencies and overlappings.

The measurement formalism concern is shared by \cite{Davis_1993}, to whom a quality software requirements specification is one that contributes to successfully, cost-effective creation of software that solver real user needs and exhibits 24 quality attributes: unambiguous, complete , correct, understandable, verifiable, internally consistent, externally consistent, achievable, concise, design independent, traceable, modifiable, electronically stored, executable/interpretable, annotated by relative importance, annotated by relative stability, annotated by version, not redundant, at right level of detail, precise, reusable, traced, organized and cross-referenced. His work also define each attribute, provide ideas on measuring them, provide a recommendation of weight relative to other attributes and describe types of activities that can be used to optimize the present of each.

Use cases quality is discussed in details by \cite{Phalp_2011}, that summarizes prior works on that area (such as the rules found by \cite{Cockburn_2000}) and proposes refined rules based on discourse process theory, such as avoiding the use of pronouns, use active voice over passive one, achieve simplicity trough avoiding to use negative forms, adjectives and adverbs and use of discourse cues and the effect of readers background and goals. Also, desirable quality attributes of use cases are listed, that may be suited for certain project phases but not others as follows: standard format, completeness, conciseness, accuracy, logic, coherence, appropriate level of detail, consistent level of abstraction, readability, use of natural language and embellishment. The authors work create rules, that should be enforced and must be obeyed, and guidelines, which indicate an ideal that cannot always be followed, that could best produce those attributes. 

ElAttar and Miller \cite{Attar_2012} present another list of use cases qualities attributes - this time, applied to use case diagrams,. They are: consistency, correctness and completeness, fault-free, analytical and understandability. Along with the attributes, the authors had performed a systematic literature review and identified 61 unique guidelines, heuristics and rules for these format of requirements, that were synthesized and compiled into a set of 21 anti-patterns, a literary form that describes a commonly occurring solution to a problem that generates decidedly negative consequences.

\subsection{Reviews and Inspections}

According to the \cite{Babok_2015}, one way to validate those characteristics is trough a review, as they can help identify defects early in the work product life cycle, eliminating the need for expensive removal of defects discovered later in the life cycle. 

Melo et. al \cite{Melo_2001} state that software review activities can be applied at many points during the software development process and can be used to discover defects in any type of deliverables or internal work products.Thus, software review activities have the "purification effect" of software artifacts as an objective. Inspection is a form of review, a rigorous defect detection process. The advantages of this inspection review process are: there is an effective mistake detection mechanism; qualitative and quantitative project feedback is given earlier; and a record of the inspection is kept, allowing the evaluation of the task performed. 

Laitenberger \cite{Laitenberger_2002} expects that inspection results depend on inspection participants themselves and their strategies for understanding the inspected artifact. Therefore, supporting inspection participants, that is, inspectors, with particular techniques that help them detect defects in software products may increase the effectiveness of an inspection team. Such techniques are referred as \textbf{reading techniques}. A reading technique can be defined as a series of steps or procedures whose purpose is to guide an inspector in acquiring a deep understanding of the inspected software product. The comprehension of inspected software products is a prerequisite for detecting subtle and/or complex defects, those often causing the most problems if detected in later life-cycle phases. In a sense, a reading technique can be regarded as a mechanism or strategy for the individual inspector to detect defects in the inspected product \cite{Laitenberger_2002}.

\subsection{Reading Techniques}

Reviews can be either formal or informal \cite{Babok_2015}. One technique to conduct formal reviews are inspections, that includes an overview of the work product, individual review, logging the defects, team consolidation of defects, and follow-up to ensure changes were made. Ad-hoc reviews, on the other hand, is an informal technique in which the business analyst seeks informal review or assistance from a peer. Formal walkthrough (also known as team review) is another type of a formal technique that uses the individual review and team consolidation activities often seen in inspection and are used for peer reviews and for stakeholder reviews. Also, single issue review (also known as technical review) is a formal technique focused on either one issue or a standard in which reviewers perform a careful examination of the work product prior to a joint review session held to resolve the matter in focus. Additionally, informal walkthrough is an informal technique in which the business analyst runs through the work product in its draft state and solicits feedback and where reviewers may do minimal preparation before the joint review session. Finally, desk check is an informal technique in which a reviewer who has not been involved in the creation of the work product provides verbal or written feedback and pass around is another an informal technique in which multiple reviewers provide verbal or written feedback. The work product may be reviewed in a common copy of the work product or passed from one person to the next.

Melo et. al \cite{Melo_2001} describes many types of reviews. Formal review is accomplished directly by the customer, with the objective of providing client feedback to the developer. Internal review is the oldest method of project review, where developers distributes copies of the project to many people, chosen by the developer himself. Walkthrough is a review process that focuses on a list of problems and, through consensus, resolves them. A software inspection is the most detailed way to perform a review, as all documentation is thoroughly studied and inspected in the process many phases and necessary corrections are made in each phase in order for the process to be completed. 

Laitenberger \cite{Laitenberger_2002} found out that ad-hoc reading and checklist-based reading are the most popular reading techniques used today for defect detection in inspection while performing a systematic review of software inspection technologies.

Ad-hoc reading offers very little reading support at all since a software product is simply given to inspectors without any direction or guidelines on how to proceed through it and what to look for. It does not, however, mean that inspection participants do not scrutinize the inspected product systematically. The word "ad-hoc" refers to the fact that no technical support is given to reviewers for the problem of how to detect defects in a software artifact and defect detection fully depends on the skill, the knowledge, and the experience of them. Training sessions may help subjects develop some of these capabilities to alleviate the lack of reading support.

Checklists offer stronger support in the form of questions, concern quality aspects of the document, inspectors are to answer while reading the document. Although reading support in the form of a list of questions is better than none, the authors debate, checklist-based reading has several weaknesses, as follows: the questions are often general and not sufficiently tailored to a particular development environment; concrete instructions on how to use a checklist are often missing, that is, it is often unclear when and based on what information an inspector is to answer a particular checklist question; questions are often limited to the detection of defects that are based on past defect information. Some principles are provided  to address some of the difficulties, and other techniques were created, being PBR a scenario based reading technique one of them.

A reading technique is a series of steps for the individual analysis of a software product to achieve the understanding needed for a particular task, increasing the effectiveness reviewers by providing guidelines that they can use to read a given software document and identify defects \cite{Melo_2001}. The authors conclude that techniques attempt, to capture knowledge about best practices for defect detection into a procedure that can be followed. In their study, perspective based reading (PBR) technique, suited for the analysis of software written documentation that we want to achieve here, is described.

PBR exploits the observation that different information in the requirement is more or less important for the different uses of the document. Thus, each reviewer on a team is asked to take the perspective of a specific user of the requirements being reviewed (a tester building a test plan, a system designer building a a design, a customer evaluating if his needs are met). The technique is designed to help reviewers answer the following questions about the requirements they are inspecting:

\begin{itemize}
    \item How do I know what information in these requirements is important to be verified?
    \item Once I have found the important information, how do I find defects in that information?
\end{itemize}

\section{Agile Requirements}

Agile software development methods take a different approach to requirements engineering and communication than the traditional requirement engineering approaches that is mostly based on formal documents and defined phases \cite{Heikkila_2015}. Trough the mapping of a systematic literature reviews on the subject, the authors have pointed out some benefits and challenges for requirement engineering on agile contexts. 

Due to this Research Plan focus on written documentation quality, two important challenges to note are the insufficiency of the user story format and the reliance on tacit requirements knowledge. The first challenge comes from the fact that user stories do not convey enough information for software design and separate systems and subsystem are required to fill that hole. The second challenge is due to the fact that the most requirements knowledge is tacit. The next sections will describe user stories, to help on the understanding of those problems, and acceptance tests, a way to express requirements details that do not fit the user story model. 

The need to better describe those tests and how they bound together with user stories is reinforced by \cite{Bjarnason_2016}, who described how tests are used as requirements on agile contexts. One of the companies on that research uses a behaviour-driven development approach, where requirements are documented upfront as automated acceptance test cases as part of the elicitation process and are expressed in a structured format in a way that the specification can be executable. This approach will be explained in details on the next sessions as well.

\subsection{User Stories}

Traditional requirements engineering activities have become too abstract and moved away from how people ordinarily learn and communicate - too far away from storytelling, something that everyone understands intuitively \cite{Rinzler_2009}. By using storytelling, the process of gathering information and structuring the requirements document would be immediately improved, as the author understands that one instinctively transform abstract knowledge into a logical structure when telling a story. 

Agile methodologies are sympathetic with this thought and represents requirements using user stories. This form of requirement's representation have been created along with the extreme programming (XP) methodology, where each story describes one thing that the system needs to do as described by \cite{Jeffries_2000}. They're written by a customer (or a customer team) to a development team, who have the responsibility to understand the story and design, build and test a software that implement it. User Stories brings together the rights of customers (who have the right to get the most possible value out of every programming moment, by asking for small atomic bits of functionality) and developers (who have the right to know what is needed). They're a short description of the behavior of the system from the point of view of the user of the system and are backed up with conversation and perhaps with some related detailed information. 

Furthermore, a story represents a feature customers want in the software, a story they would like to be able to tell their friends about this great system they are using \cite{Beck_Fowler_2000}. They're some specific things that would make the system easy to use, a chunk of functionality that is of value to the customer. As a unit of functionality, the team demonstrate progress by delivering tested, integrated code that implements a story. The customer must write the story in cards, and it  is up to the developers to estimate it trough collaboration with the customer.

The main format of a user story, popularized on \cite{Cohn_2004}, is shown below:

\begin{framed}

\center I as a (role) want (function) so that (business value)

\end{framed}

For \cite{Cohn_2004}, a user story describes functionality that will be valuable to either a user or purchaser of a system or software. Each story must be written in the language of the business, not in technical jargon, so that the customer team can prioritize the stories for inclusion into iterations and releases. The author provides some reasons on why user stories are used, as follows: they emphasize verbal rather than written communication; are comprehensible by both the customer and the developers and encourage; and encourage deferring detail until the team, customers and developers, have the best understanding they are going to have about what they really need.

%need Scrum view of user stories ?

Lucassen et. al \cite{Lucassen_2015} summarize that user stories only capture the essential elements of a requirement: \textit{who} it is for, \textit{what} it expects from the system, and, optionally, \textit{why} it is important. As user stories express \textit{what} is desired and \textit{why} it is needed by the client, they are better compared to business or stakeholders requirements from BABOK (\cite{Babok_2009} and \cite{Babok_2015}).

\subsection{Acceptance Tests}

While Jeffries et. al \cite{Jeffries_2000} list only two aspects of user stories (the short description of it and the conversations around it), Cohn \cite{Cohn_2004} composes a User Story using three aspects:

\begin{itemize}
    \item a written description of the story used for planning and as a reminder;
    \item conversations about the story that serve to flesh out the details of the story;
    \item tests that convey and document details and that can be used to determine when a story is complete.
\end{itemize}

Those aspects have come from the Card, Conversation and Confirmation terms defined by \cite{Jeffries_2001}. He described the Card represent customer requirements rather than document them, has just enough text to identify the requirement, and to remind everyone what the story is. The Conversation is an exchange of thoughts, opinions, and feelings. It is largely verbal, but can be supplemented with documents. The best supplements are examples and the best examples should be executable. They're representations of the Confirmation, a way to customers tell to developers how she will confirm that they've done what is needed in the form of acceptance tests. That confirmation, provided by those examples, is what makes possible the simple approach of card and conversation. When the conversation about a card gets down to the details of the acceptance test, the customer and programmer settle the final details of what needs to be done.

Acceptance testing is the process of verifying that stories were developed such that each works exactly the way the customer team, the ones who write user stories on XP methodology, expected it to work \cite{Cohn_2004}. They're specified as soon as the iteration begins and, depending on the customer team's technical expertise, can be put into an automated testing tool. Those tests helps the customer team to communicate assumptions and expectations to the developers, by expressing the details that result from the conversations between customers and developers, and also validate that a story has been developed with the functionality the team had in mind when they wrote the story. New tests should be added as long as they add value and clarification to the story. Their scope should focus on clarifying the intent of the story to the developers and not cover all the low level validations that should happen (like invalid dates range).

Jeffries et. al \cite{Jeffries_2000} said that acceptance tests allow the customer to know when the system works, and tell the programmers what needs to be done. On XP methodology, programmers have the right to know what is needed and customers have the right to see progress in a running system, proven to work by automated test that you specify. They provide confidence that the system really does what it needs to do.

For \cite{Beck_Fowler_2000}, acceptance tests execution will determine whether the user stories have been successfully implemented. Also, once acceptance tests are running, the story can be discarded, since they're encoded in far more detail and accuracy in the acceptance tests, so no information will be lost if the cards are destroyed.

Gartner \cite{Gartner_2012} said that the hardest job in software is communicating clearly about what we want the system to do and driving the development effort with acceptance tests helps with the challenge. Two core practices are described on his book, as follows: (a) before implementing each feature, team members collaborate to create concrete examples of the feature in action and (b) then the team translates these examples into automated acceptance tests, that become a prominent part of the team's shared, precise description of "done" for each feature. Teams that follow those practices are working on a Acceptance Test-Driven Development (\textbf{ATDD}) way.

Haugset and Stalhane \cite{Haugset_2012} describe ATDD as a mixture of the documentation centric RE and the communication-focused iterative agile RE, carrying its own set of benefits and challenges. The benefits stem both from the inherent strengths that lie in discussing requirements using the acceptance tests as a mediator and its abilities for automation of the same tests. On the case study described by the authors, the technique was used to help communicate requirements (externalization) in the specification phase, a process had nothing to do with automation but with uncovering, understanding and describing requirements. In the verification phase ATDD was used to automatically verify that the code adhered to the customer requirement on an acceptance level, accompanied by manual testing for certain issues. The authors judge that ATDD saves effort in the long run, despite having a higher upfront cost due to the need of even more interaction with customers than agile development does, since the tests act as a safety harness for making changes and acts as documentation of code.

As Acceptance Tests describes the expectations that a system must show in order to demonstrate that the application is acceptable by the customer \cite{Cohn_2004}, they can fill the documentation role of functional requirements from \cite{Babok_2009} and \cite{Babok_2015}. Yet, due to the fact that they're often automated test and often written by customers with their own words, they can also fill the living documentation vision proposed by \cite{Adzic_2011}.

\subsection{Behavior Driven Development}

One way to represent acceptance tests are scenarios. Alexander and Maiden \cite{Alexander_2004} describe that the activity of building requirements scenarios encourages imagination and exploration and sets the stage for discovering unconscious and undreamed of requirements and that it is important because stories are the primary
(perhaps only) means of communicating needs and desires, and providing critical feedback, to developers.

Kaner \cite{Kaner_2003} describes scenarios as a hypothetical story used to help a person think through a complex problem or system. Scenarios also helps to learn the product, as people don’t learn well by following checklists or material that is organized for them - they learn by doing tasks that require them to investigate the product for themselves. The author also brings the idea that a scenario is an instantiation of a use case—take a specific path through the model, assigning specific values to each variable. Finally, as the scenario is a story about someone trying to accomplish something with the product under test, it can help to expose failures to deliver desired benefits.

Scenarios can also represent the key examples that describe the expected functionality, a concept introduced by \cite{Adzic_2011}. The living documentation presented by the author helps to validate if the system works in the same way reflected by the documentation that represents it. Example tables are also used on the mentioned book, where one represents the system's inputs and outputs using tables where each row represents a given example. Specialized tools like the framework for integrated tests shown by \cite{Gartner_2012} can read those tables and use them on the software under test. 

%need to explain Fit ?

%bdd introduction
Behavior-Driven Development (\textbf{BDD}) is a set of practices that uses scenarios as an ubiquitous language to describe and model a system \cite{Smart_2014}. Scenarios are expressing in a format known as Gherkin, that is designed to be both easily understandable for business stakeholders and easy to automate using dedicated tools. According to the author, bringing business and technical parties together to talk about the same document helps to build the right software (the one that meets customer needs) and to build it right (without buggy code). Those scenarios are referred as structured examples, in a similar way as Adzic \cite{Adzic_2011} calls them key examples. Many ideas are similar on both publications. As explained by \cite{Smart_2014}, using conversation and examples to specify how you expect a system to behave is a core part of BDD. 

% bdd == acceptance tests
BDD practitioners collaborates with the user to find concrete examples to illustrate features requested to the development team. Quite often, they also automate the execution of those scenarios. Since they are written to express the details and help the customer accept a given feature, they can be used as acceptance tests. Also, the collaboration among development team and customers that is needed to write those scenarios can be mapped as the conversations about a story that \cite{Cohn_2004} and \cite{Jeffries_2001} talked about.

In Gherkin, the requirements related to a particular feature are grouped into a single text file called a feature file. A feature file contains a short description of the feature, followed by a number of scenarios, or formalized examples of how a feature works. Each scenario is made up of a number of steps, where each step starts with one of a small number of keywords. The natural order of a scenario is \textit{Given...} \textit{When...} \textit{Then...}, such as:
\begin{itemize}
    \item Given describes the preconditions for the scenario and prepares the test environment;
    \item When describes the action under test;
    \item Then describes the expected outcomes.
\end{itemize}

One example of a scenario for a train management application that should have a feature to help a commuter to find the optimal itinerary between stations on the same line found by \cite{Smart_2014}:

\begin{framed}
\textbf{Given} Western line trains from Emu Plains leave Parramatta for Town Hall at 7:58, 8:00, 8:02, 8:11

\textbf{When} I want to travel from Parramatta to Town Hall at 8:00

\textbf{Then} I should be told to take the 8:02 train
\end{framed}

% bdd == use cases == functional requirements.
BDD scenarios are similar to use cases scenarios as they both describe a system behavior under certain precondition (expressed on the \textit{Given} clauses of a BDD scenario) to achieve a certain goal (expressed on the \textit{Then} clauses of a BDD scenario). As they're a format to express acceptance tests, they can also fill the documentation role of functional requirements from \cite{Babok_2009} and \cite{Babok_2015}. Finally, due to the fact that they're often automated test and are written on a ubiquitous language that should be understood by everyone in the project, they can also fill the living documentation vision proposed by \cite{Adzic_2011}.

\subsection{Agile Requirements Quality}

According to \cite{Lucassen_2015}, the number of methods to assess and improve user story quality is limited. Existing approaches to user story quality employ highly qualitative metrics, such as the heuristics of the INVEST (Independent-Negotiable-Valuable-Estimable-Scalable-Testable) framework described by \cite{Cohn_2004}. Due to that fact, \cite{Lucassen_2015} define additional criteria to evaluate user stories on their QUS Framework, as follows: atomic, minimal, well-formed, conflict-free, conceptually sound, problem-oriented, unambiguous, complete, explicit dependencies, full sentence, independent, scalable, uniform and unique.

For acceptance tests, our intuition is that they should meet the quality attributes that all requirements from \cite{Babok_2009} and \cite{Babok_2015} meet, as we compare acceptance tests with functional requirements, and the ones for use cases found by \cite{Cockburn_2000} and \cite{Phalp_2011}. However, little work has been found to support this intuition. 

%generic scenarios characteristics
Generic scenarios quality evaluation seems to be based upon subjective characteristics, like the ones found by \cite{Kaner_2003}, or on generic guidelines, like what Alexander and Maiden \cite{Alexander_2004} had highlighted. Kaner \cite{Kaner_2003} describes that a test based in a scenario has five characteristics, as follows: it is (a) a story that is (b) motivating, (c) credible, (d) complex, and (e) easy to evaluate. Nevertheless, empirical evaluation of existing acceptance test cases expressed as scenarios according to his characteristics were not found. Alexander and Maiden \cite{Alexander_2004} define a guideline that should be use for use cases and scenarios alike - his view seems to indicate they fill the same role on requirements documentation.  

BDD scenarios quality attributes, on the other hand, can be only evaluated based on subjective characteristics, such as those described by \cite{Smart_2014}: the scenarios steps expressiveness, focused on what goal the user want to accomplish and not on implementation details or on screen interactions (writing it in a declarative way and not on a imperative way); the use of preconditions on the past tense, to make it transparent that those are actions that have already occurred in order to begin that test; the reuse of information to avoid unnecessary repetition of words; and the scenarios independence. The author specify examples of good and bad scenarios in order to demonstrate those characteristics.

\section{Conclusion}

In order to summarize the content of this chapter, we judge it necessary to directly answer the informal questions made on the beginning of it. 

The quality characteristics shown on \cite{Babok_2009} and \cite{Babok_2015} have taught us that traditional requirements writers have many guides to help them evaluate their work like the ones described by \cite{Laitenberger_2002} and \cite{Melo_2001}. Also, \cite{Phalp_2011} have shown that use cases quality criteria and guidelines are also mature enough to help practitioners writing them. Finally, user stories writers have informal guidelines (such as the INVEST acronym used by \cite{Cohn_2004}) and an ongoing study on quality characteristics (from \cite{Lucassen_2015}) to help on their efforts. On the other hand, the quality of acceptance tests is still open to debate, with only a few characteristics (found by \cite{Kaner_2003} and \cite{Smart_2014}) or generic guidelines (like the one created by \cite{Alexander_2004}) to use, all without empirical evaluation to certify their usefulness. No inspection technique addresses acceptance tests as well, on the best of our knowledge.  

On agile contexts, requirements are represented mainly as user stories according to \cite{Lucassen_2015}. Due to that fact, it makes sense that the authors focus their quality framework on the user story card representation from \cite{Cohn_2004} only, letting the details that are expressed as acceptance tests out of it. However, due to the growing importance of requirements expressed as tests shown by \cite{Bjarnason_2016}, we believe this gap need to be filled.

BDD is a format to represent acceptance tests (as stated by \cite{Gartner_2012}) that was used on one of the companies interviewed by \cite{Bjarnason_2016} study and to which only informal characteristics (found by \cite{Smart_2014}) exists to evaluate them. Since BDD scenarios represent the intended behavior of a system, the quality attributes for use cases and functional requirements could be re-used - but no study was found on that area. Also, the framework from \cite{Lucassen_2015} could be expanded to also cover BDD scenarios - but again, no efforts on that direction have been found. Finally, having a set of attributes to evaluate a written work without a strategy to do that may not be enough to help the writer effectively. Thus, there's also the need to provide a way to guide the software development team members on how to proper use those attributes in a review process.