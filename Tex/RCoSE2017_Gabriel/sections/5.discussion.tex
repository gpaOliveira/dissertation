\section{Perceptions about the Study}

%gabriel - menciona um pouco aqui sobre os tipos de coleta de dados que fizemos - done, re-lembrei o leitor das tasks 1-4
The above mentioned study has finished its execution phase, with all the 15 students having performed their four assigned tasks, but the analysis of the data collected throughout the completion of tasks 1 to 4 and the students interpretation about quality attributes is still on-going. However, we could already take some degree of insight from our notes during the study and from a final 2 hours long discussion round with the students once they concluded performing their work.  

%sabrina - acho que pra dizer isto, precisamos dizer que nos provemos a lista de atributos para guiar a analise. - done I guess, mencionei na parte de 'design' do 4.study
First of all, the list of chosen attributes generated confusion on the evaluation of BDD scenarios. As it mixed traditional requirements characteristics from both versions of the BABOK \cite{Babok_2009} \cite{Babok_2015} and the INVEST heuristic provided by Cohn \cite{Cohn_2004}, some students could not see some attributes as different. For example, atomicity from BABOK \cite{Babok_2015} and independence from INVEST \cite{Cohn_2004} were often seen as opposites, even if this is not always the case. One student reported that this confusion may have come from the difficulty to see bad examples of BDD scenarios, those specially built to demonstrate when an attribute fail.

Secondly, some attributes may not make sense to evaluate a single BDD scenario - completeness may represent how a set of scenarios demonstrate a complete set of examples to cover a user story, for example. As it was said before, we let the scope of the analysis open to interpretation on purpose. Thus, five students out of the 15 have joined the scenarios from a user story together before analyzing them, witch raises questions on what were the reasons that drove their decision. Follow up sessions need to be scheduled with those studies to search for the reasoning behind their evaluation approach. 

Thirdly, the students reported that inputs/outputs are important on BDD scenarios to help on prioritization and testability. As they do no have use cases implementation details to help the development team estimate and analyze the new feature impact and on the system, using inputs/outputs on scenarios helps to improve prioritization and testability. Also, a few of them see that the lack of inputs/outputs can be seen as a lack of completeness attribute. Those perceptions are similar to the good examples given by Smart \cite{Smart_2014} on his book - however, the lack of input/output is not explicitly provided by him.

Fourthly, written rigour was judged as necessary by the students to re-enforce the team's common understanding during conversations with customers. As they stated, the writing of BDD scenarios is easier (due to the use of plain English language to represent behaviours) and could theoretically be done by anyone - however, they reported that someone who provides good examples is desirable to better demonstrate, through those example, to the development team how a new functionality can be put together with the existent ones. 

\section{Conclusion \& Future Work}
%sabrina - otimo, mas em nenhum momento mencionamos checklists acima. - done, reforcei o "pre-defined list of quality attributes"
%sabrina - excelente paragrafo, mas tem duas coisas aqui. 
% --- 1) parece que a conclusao eh sobre usar checkists - done, reforcei o "pre-defined list of quality attributes"
% --- 2) with acceptance test scenarios. - done, mudado para BDD scenarios or other types of acceptance test e reforcei o link entre BDD <- acceptance tests com a inclusÃ£o de Gartner_2012

Our study perceptions seems to reinforces our belief that pre-defined list of quality attributes is important to guide the quality evaluation of BDD scenarios. A detailed data analysis is needed to confirm that thought. Still, those perceptions opened the debate questioning if a list of attributes alone is useful to achieve that goal or if other formats of checklist (e.g: direct questions, as those developed to use cases by Cockburn \cite{Cockburn_2000}) will yield better results. Additionally, the problem may be centered on the list of attributes itself - we know that some of them can be used to evaluate other requirements formats, but should they be used with BDD scenarios or other types of acceptance tests ? For example, execution stability is never mentioned as a quality concern, even with the known problems with flaky tests reported by Neely and Stolt \cite{Neely_Stolt_2013}. Therefore, the concerns that 

Therefore, our next steps to achieve our goal are to better understand how acceptance tests are evaluated, what problems may appear during their creation or use on a continuous engineering context and map those problems on BDD scenarios. This knowledge will give us confidence to build a question based checklist to avoid those problems.