\section{Background}

\subsection{Behavior-Driven Development as a Continuous * Practice}
Behavior-Driven Development is an umbrella term that encapsulates a set of practices that uses scenarios as a ubiquitous language to describe and model a system \cite{Smart_2014}. Scenarios are expressed in a format known as Gherkin, that is designed to be both easily understandable for business stakeholders and easy to automate using dedicated tools. Smart \cite{Smart_2014} argues that bringing business and technical parties together to talk about the same document helps to build the right software (the one that meets customer needs) and to build it right (without buggy code). He also says that using conversation and examples to specify how one expects a system to behave is core to BDD. 

It's a known fact that BDD scenario is a format to represent acceptance tests \cite{Gartner_2012}, as it fulfills the role of the Confirmation term defined by Jeffries \cite{Jeffries_2001}. He described that the Card (most commonly written in user story format on agile methodologies) represent customer requirements rather than document them, has just enough text to identify the requirement and to remind everyone what the story is. The Conversation is an exchange of thoughts, opinions, and feelings. It is largely verbal but can be supplemented with documents. The best supplements are examples and the best examples should be executable. They are representations of the Confirmation, a way to customers tell to developers how she will confirm that they have done what is needed in the form of acceptance tests. That Confirmation, provided by those examples, is what makes possible the simple approach of card and conversation. When the conversation about a card gets down to the details of the acceptance test, the customer and programmer settle the final details of what needs to be done. When the iteration ends and the programmers demonstrate the acceptance tests running, the customer learns that the team can, and will, deliver what is needed.

The importance of acceptance tests execution to Continuous Integration is well known. Humble and Farney \cite{Humble_Farley_2010} states that the cost of properly created and maintained automated acceptance test suite is much lower than that of performing frequent manual acceptance and regression testing, or that of the alternative of releasing poor-quality software. Since BDD scenarios are easily automated by tools like JBehave\footnote{https://jbehave.org/} and Cucumber\footnote{https://cucumber.io/}, the scenarios created and maintained can be called executable specifications \cite{Smart_2014} \cite{Humble_Farley_2010}. 

For teams practicing BDD, the requirements and executable specifications are the same thing \cite{Smart_2014}. When the requirements change, the executable specifications are updated directly in a single place. Therefore, it helps to lower the burden of constantly updates on test scenarios when changes on the specification happens, thus making it easier for the team to have a Continuous Testing mindset, a desirable culture pattern described by Fitzgerald and Stol \cite{Fitzgerald_Stol_2014}.
 
The use of acceptance tests as documentation is also highlighted by Neely and Stolt \cite{Neely_Stolt_2013} on their experience at Rally Software. When planning stories that require modification of existing code, they state that the lengths of documenting existing tests can be used as guidance when discussing the story details. It helps to enhance the team's visibility about code's test coverage and also allows QA and developers to close existing gaps, boosting the team level of confidence on that part of the application. As the use of acceptance tests can tighten the integration between planning and execution, it could help to fulfill the vision of Continuous Planning described by Fitzgerald and Stol \cite{Fitzgerald_Stol_2014}.

However, little attention is being given to the quality of that written documentation on BDD scenarios format. To the best of our knowledge, the only guide practitioners have to validate their scenarios are taken from tips based on few examples described by Smart \cite{Smart_2014} in his book, as follows: the scenarios steps expressiveness, focused on what goal the user want to accomplish and not on implementation details or on screen interactions (writing it in a declarative way and not on an imperative way); the use of preconditions on the past tense, to make it transparent that those are actions that have already occurred in order to begin that test; the reuse of information to avoid unnecessary repetition of words; and the scenarios independence. The author specifies examples of good and bad scenarios in order to demonstrate those characteristics.  

We believe that, if the Confirmation is not a good representative of the details discussed in Conversations by the team and the customer, the simple approach of writing customer needs on Cards is not effective. With the lack of criteria to validate acceptance tests on BDD scenarios format, we look upon other requirements formats.

\subsection{Quality Criteria for other Requirements Formats}
%Sabrina - titulo todo com as primeiras letras em maisuculo. - done

%Sabrina - gabriel, revisa a frase aqui - done, reformulated 
The Business Analyst Body of Knowledge (BABOK) newest edition \cite{Babok_2015} states that while quality is ultimately determined by the needs of the stakeholders who will use the requirements or the designs, acceptable quality requirements exhibit many characteristics. It lists some characteristics a requirement must have in order to be a quality one, as follows: atomic, complete, consistent, concise, feasible, unambiguous, testable, prioritized, and understandable. A slightly different list is found on a prior version \cite{Babok_2009}, as follows: cohesion, completeness, consistency, correction, viability, adaptability, unambiguity, and testability. Although the characteristics' meaning is defined, no measurement guidance is given.

Cockburn \cite{Cockburn_2000} seems to take inspiration on those attributes to define rules on how to validate use cases, a requirement format that captures a contract between the stakeholders of a system about its behavior and describes the systemâ€™s behavior under various conditions by interacting with one of the stakeholders (the \textit{primary actor}, who want to perform an action and achieve a certain goal). Those rules are summarized in a pass/fails questionnaire to be applied on use cases - good use cases are those that yield an "yes" answer to all of them.

Use cases quality is also discussed in details by Phalp and colleagues \cite{Phalp_et_dot_al_2011} who summarize prior work on that area and propose refined rules based on discourse process theory, such as avoiding the use of pronouns, use active voice over passive one, achieve simplicity trough avoiding to use negative forms, adjectives and adverbs and use of discourse cues and the effect of readers background and goals. Also, desirable quality attributes of use cases are listed, that may be suited for certain project phases but not others as follows: standard format, completeness, conciseness, accuracy, logic, coherence, the appropriate level of detail, consistent level of abstraction, readability, use of natural language, and embellishment. With those quality attributes in mind, the authors create rules, that should be enforced and must be obeyed, and guidelines, which indicate an ideal that cannot always be followed, that could best produce those attributes.

Most agile methodologies tend to not use traditional requirements or use cases, but represents requirements using user stories, that fill the Card role described by Jeffries \cite{Jeffries_2001}. For Cohn \cite{Cohn_2004}, a user story describes functionality that will be valuable to either a user or purchaser of a system or software. Lucassen et. al \cite{Lucassen_et_dot_al_2015} summarize that user stories only capture the essential elements of a requirement: \textit{who} it is for, \textit{what} it expects from the system, and, optionally, \textit{why} it is important.

Lucassen et. al \cite{Lucassen_et_dot_al_2015} argue that the number of methods to assess and improve user story quality is limited. Existing approaches to user story quality employ highly qualitative metrics, such as the heuristics of the INVEST (Independent-Negotiable-Valuable-Estimable-Scalable-Testable) framework described by Cohn \cite{Cohn_2004}. Due to that fact, Lucassen et. al \cite{Lucassen_et_dot_al_2015} define additional criteria to evaluate user stories on their QUS Framework, as follows: atomic, minimal, well-formed, conflict-free, conceptually sound, problem-oriented, unambiguous, complete, explicit dependencies, full sentence, independent, scalable, uniform, and unique.
