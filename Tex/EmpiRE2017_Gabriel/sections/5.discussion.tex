\section{Perceptions about the Study}

The study has finished its execution phase, with all the 15 students having performed their four assigned tasks, but the analysis of the data collected throughout the completion of tasks 1 to 4 and the students interpretation about quality attributes will still be crossed with practitioners opinions about what makes a "good" BDD scenario. We could already take some degree of insight from our notes during the study and from a final 2 hours long discussion round with the students once they concluded performing their work.

When inquired about comparing their work with different requirement formats, the students reported that BDD scenarios are easier to write (due to the lack of description of every user interactions with the system) but hard to analyze when compared to use cases. Due to that fact, BDD scenarios description were found to be more negotiable then use cases (one of the INVEST attributes). In the other hand, use cases were found to be more easily testable. It's interesting to note their assumption that use cases descriptions need to be more imperative (with details explicitly defined) and scenarios descriptions need to be declarative (without implementation details) - one we did not foreseen.

Also, the list of chosen attributes generated confusion on the evaluation of BDD scenarios. As it mixed traditional requirements characteristics from both versions of the BABOK \cite{Babok_2009} \cite{Babok_2015} and the INVEST acronym provided by Cohn \cite{Cohn_2004}, some students did not understood how to proper differentiate some attributes when evaluating scenarios. For example, atomicity from BABOK \cite{Babok_2015} and independence from INVEST \cite{Cohn_2004} were often seen as opposites, even if this is not always the case - a requirement that specifies only one action (atomic) can have many or none dependencies with others. One student reported that this confusion may have come from the difficulty to see bad examples of BDD scenarios, specially built to demonstrate when an attribute fail. They did not reported about problems to use INVEST acronym on use cases, tough.

Some attributes may not make sense to evaluate a single BDD scenario - completeness may only represent how a set of scenarios cover a user story, for example. As it was said before, we let the scope of the analysis open to interpretation on purpose. Thus, five students out of the 15 have joined the scenarios from a user story together before analyzing them, witch raises questions on what were the reasons that drove their decision. Follow up sessions need to be scheduled with those students to search for the reasoning behind their evaluation approach. 

The students also reported that inputs/outputs are important on BDD scenarios to help on prioritization and testability. As they don't have use cases' imperative description of every user interactions with the system to help them understand the technical steps needed to perform an action, stating inputs and outputs clearly may impact the ability of the development team to estimate effort and analyze impact, thus impacting on the testability attribute. A few of them understands that the lack of inputs/outputs can be seen as a lack of the completeness attribute. Those perceptions are similar to the good examples given by Smart \cite{Smart_2014} on his book, although the lack of input/output is not explicitly provided by him as a good/bad thing.

Lastly, written rigor was judged as necessary by the students to re-enforce the team's common understanding during conversations with customers. As they stated, the writing of BDD scenarios is easier that use cases (due to the use of plain English language to represent behaviors and the declarative description format) and could be done by anyone - however, they reported that just someone who provides good examples would be desirable, in order to express the different ways a new functionality can be combined with existent ones. In a similar way, only have technical writers should be working with use cases - to proper describe the use case in an imperative way.

\section{Conclusion \& Future Work}

Our study perceptions so far seems to reinforces the belief that a list of quality attributes can be used to guide the quality evaluation of BDD scenarios. However, our initial choice of attributes revealed that some may not be suited for BDD scenarios individually (like completeness) or may be only seen as a confusion source to the evaluator (like atomicity or independence). By analyzing the students evaluations of each other scenarios using a content analysis method \cite{White_Marsh_2006}, we came up with a list of synonyms for each quality attribute used on BDD scenarios - for example, for non-ambiguity, we came up we words like "direct" or "clear" while for prioritized we got "importance" or "ranking". 

One next step on our research is to acquire practitioners opinions about what makes "good" BDD scenarios and cross them with those synonyms, to map them back to existing attributes or provide us with words clusters to use as new ones. Another related next step is to ask those same practitioners if attributes lists are indeed a good way to validate BDD scenarios, as the confusion on understand the presence/absence of an attribute showed by students may reveal the need to evaluate scenarios with direct questions similar to those developed by Cockburn \cite{Cockburn_2000} to use cases.